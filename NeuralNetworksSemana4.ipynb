{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/outofcore_modelpersistence.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDb Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train a simple logistic regression model to classify movie reviews from the 50k IMDb review dataset that has been collected by Maas et. al.\n",
    "\n",
    "> AL Maas, RE Daly, PT Pham, D Huang, AY Ng, and C Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin- guistics: Human Language Technologies, pages 142â€“150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics\n",
    "\n",
    "[Source: http://ai.stanford.edu/~amaas/data/sentiment/]\n",
    "\n",
    "The dataset consists of 50,000 movie reviews from the original \"train\" and \"test\" subdirectories. The class labels are binary (1=positive and 0=negative) and contain 25,000 positive and 25,000 negative movie reviews, respectively.\n",
    "For simplicity, I assembled the reviews in a single CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I've expected a comedy about the NVA, but this...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Jamie Foxx was the epitome of Ray Charles. Aft...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>The last couple of weeks in the life of a dead...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>(No need to recap the plot, since others have ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>The movie starts quite with an intriguing scen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  I've expected a comedy about the NVA, but this...          0\n",
       "49996  Jamie Foxx was the epitome of Ray Charles. Aft...          1\n",
       "49997  The last couple of weeks in the life of a dead...          0\n",
       "49998  (No need to recap the plot, since others have ...          1\n",
       "49999  The movie starts quite with an intriguing scen...          0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# if you want to download the original file:\n",
    "#df = pd.read_csv('https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/50k_imdb_movie_reviews.csv')\n",
    "# otherwise load local file\n",
    "df = pd.read_csv('shuffled_movie_data.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us shuffle the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## uncomment these lines if you have dowloaded the original file:\n",
    "#np.random.seed(0)\n",
    "#df = df.reindex(np.random.permutation(df.index))\n",
    "#df[['review', 'sentiment']].to_csv('shuffled_movie_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define a simple `tokenizer` that splits the text into individual word tokens. Furthermore, we will use some simple regular expression to remove HTML markup and all non-letter characters but \"emoticons,\" convert the text to lower case, remove stopwords, and apply the Porter stemming algorithm to convert the words into their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it at try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', ':)', ':)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('This :) is a <a> test! :-)</br>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning (SciKit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a generator that returns the document body and the corresponding class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conform that the `stream_docs` function fetches the documents as intended, let us execute the following code snippet before we implement the `get_minibatch` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"Not worth the video rental or the time or the occasional efforts.<br /><br />*Makeup that a child can do. *Acting was over done...poor directing. *Editing was very choppy...many things made no sense or just seemed gratuitous. *Sound was badly dubbed. *Music was highly inappropriate. *Casting was extremely off...must have been on crack. *Zombies that talk let alone...drive, dance, work...just pisses me off. *And the bad guy...Holy Crap! As horribly casted as he was...he was the best looking zombie of all. Which doesn\\'t say much.<br /><br />The Cover Art was good but very deceiving...as was the Main Menu of the DVD...great artwork and music.<br /><br />DON\"\"T BOTHER!\"',\n",
       " 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='shuffled_movie_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we confirmed that our `stream_docs` functions works, we will now implement a `get_minibatch` function to fetch a specified number (`size`) of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    for _ in range(size):\n",
    "        text, label = next(doc_stream)\n",
    "        docs.append(text)\n",
    "        y.append(label)\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will make use of the \"hashing trick\" through scikit-learns [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) to create a bag-of-words model of our documents. Details of the bag-of-words model for document classification can be found at  [Naive Bayes and Text Classification I - Introduction and Theory](http://arxiv.org/abs/1410.5329)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object stream_docs at 0x7f73cc3e0f10>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'at'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-89d05592cc61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"review\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'at'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Exercise 1: define features based on word embeddings (pre-trained word2vec vectors can be used)\n",
    "# Define suitable d dimension, and sequence length\n",
    "\n",
    "doc_stream = stream_docs(path='shuffled_movie_data.csv')\n",
    "print(doc_stream)\n",
    "\n",
    "X_train = []\n",
    "for i in range(50000):\n",
    "    X_train.append(tokenizer(doc_stream.at[i, \"review\"]))\n",
    "    Y_train.append(tokenizer(doc_stream.at[i, \"sentiment\"]))\n",
    "\n",
    "model = Word2Vec(X_train, size=100, window=5, min_count=5, workers=4)\n",
    "word_vectors = modWord2vec.wv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [SGDClassifier]() from scikit-learn, we will can instanciate a logistic regression classifier that learns from the documents incrementally using stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 2: Define at least a Three layer neural network. Define its structure (number of hidden neurons, etc)\n",
    "# Define nonlinear function for hidden layers.\n",
    "# Define a suitbale loss function for binary classification\n",
    "# Implement the backpropagation algorithm for this structure\n",
    "# Train the model using SGD\n",
    "\n",
    "##-----------------------------------------------------------------------------------\n",
    "##                                  CLASE RED\n",
    "##-----------------------------------------------------------------------------------\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, Num_Input, Num_Hidden, Num_Output, seed):\n",
    "        self.ni = Num_Input\n",
    "        self.nh = Num_Hidden\n",
    "        self.no = Num_Output\n",
    "\n",
    "        self.iNodes = np.zeros(shape=[self.ni], dtype=np.float32)\n",
    "        self.hNodes = np.zeros(shape=[self.nh], dtype=np.float32)\n",
    "        self.oNodes = np.zeros(shape=[self.no], dtype=np.float32)\n",
    "\n",
    "        self.ihWeights = np.zeros(shape=[self.ni,self.nh], dtype=np.float32)\n",
    "        self.hoWeights = np.zeros(shape=[self.nh,self.no], dtype=np.float32)\n",
    "\n",
    "        self.hBiases = np.zeros(shape=[self.nh], dtype=np.float32)\n",
    "        self.oBiases = np.zeros(shape=[self.no], dtype=np.float32)\n",
    "\n",
    "        self.rnd = random.Random(seed)\n",
    "        self.Init_Weights()\n",
    "\n",
    "    def Set_Weights(self, weights):\n",
    "        idx = 0\n",
    "        for i in range(self.ni):\n",
    "            for j in range(self.nh):\n",
    "                self.ihWeights[i,j] = weights[idx]\n",
    "                idx += 1\n",
    "\n",
    "        for j in range(self.nh):\n",
    "            self.hBiases[j] = weights[idx]\n",
    "            idx += 1\n",
    "\n",
    "        for j in range(self.nh):\n",
    "            for k in range(self.no):\n",
    "                self.hoWeights[j,k] = weights[idx]\n",
    "                idx += 1\n",
    "\n",
    "        for k in range(self.no):\n",
    "            self.oBiases[k] = weights[idx]\n",
    "            idx += 1\n",
    "            \n",
    "    def getWeights(self):\n",
    "        tw = self.totalWeights(self.ni, self.nh, self.no)\n",
    "        result = np.zeros(shape=[tw], dtype=np.float32)\n",
    "        idx = 0\n",
    "        \n",
    "        for i in range(self.ni):\n",
    "            for j in range(self.nh):\n",
    "                result[idx] = self.ihWeights[i,j]\n",
    "                idx += 1\n",
    "\n",
    "        for j in range(self.nh):\n",
    "            result[idx] = self.hBiases[j]\n",
    "            idx += 1\n",
    "\n",
    "        for j in range(self.nh):\n",
    "            for k in range(self.no):\n",
    "                result[idx] = self.hoWeights[j,k]\n",
    "                idx += 1\n",
    "\n",
    "        for k in range(self.no):\n",
    "            result[idx] = self.oBiases[k]\n",
    "            idx += 1\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def Init_Weights(self):\n",
    "        numWts = self.totalWeights(self.ni, self.nh, self.no)\n",
    "        Initial_W = np.zeros(shape=[numWts], dtype=np.float32)\n",
    "        lo = -0.01; hi = 0.01 # Inicializar los pesos en rangos\n",
    "        for idx in range(len(Initial_W)):\n",
    "            Initial_W[idx] = (hi - lo) * self.rnd.random() + lo\n",
    "        self.Set_Weights(Initial_W)\n",
    "        \n",
    "    def Feed_Forward(self, xValues):\n",
    "        hSums = np.zeros(shape=[self.nh], dtype=np.float32)\n",
    "        oSums = np.zeros(shape=[self.no], dtype=np.float32)\n",
    "\n",
    "        for i in range(self.ni):\n",
    "            self.iNodes[i] = xValues[i]\n",
    "\n",
    "        for j in range(self.nh):\n",
    "            for i in range(self.ni):\n",
    "                hSums[j] += self.iNodes[i] * self.ihWeights[i,j]\n",
    "\n",
    "        for j in range(self.nh):\n",
    "            hSums[j] += self.hBiases[j]\n",
    "\n",
    "        for j in range(self.nh):\n",
    "            self.hNodes[j] = self.hypertan(hSums[j])\n",
    "\n",
    "        for k in range(self.no):\n",
    "            for j in range(self.nh):\n",
    "                oSums[k] += self.hNodes[j] * self.hoWeights[j,k]\n",
    "\n",
    "        for k in range(self.no):\n",
    "            oSums[k] += self.oBiases[k]\n",
    "\n",
    "        softOut = self.softmax(oSums)\n",
    "        for k in range(self.no):\n",
    "            self.oNodes[k] = softOut[k]\n",
    "\n",
    "        result = np.zeros(shape=self.no, dtype=np.float32)\n",
    "        for k in range(self.no):\n",
    "            result[k] = self.oNodes[k]\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def train(self, trainData, Num_Epochs, Alpha):\n",
    "        hoGrads = np.zeros(shape=[self.nh, self.no], dtype=np.float32)  # Hid2Out gradients\n",
    "        obGrads = np.zeros(shape=[self.no], dtype=np.float32)  # output biases gradients\n",
    "        ihGrads = np.zeros(shape=[self.ni, self.nh], dtype=np.float32)  # In2Out gradients\n",
    "        hbGrads = np.zeros(shape=[self.nh], dtype=np.float32)  # hidden biases gradients\n",
    "\n",
    "        oSignals = np.zeros(shape=[self.no], dtype=np.float32)  # output\n",
    "        hSignals = np.zeros(shape=[self.nh], dtype=np.float32)  # hidden\n",
    "\n",
    "        epoch = 0\n",
    "        X_Values = np.zeros(shape=[self.ni], dtype=np.float32)\n",
    "        T_Values = np.zeros(shape=[self.no], dtype=np.float32)\n",
    "        numTrainItems = len(trainData)\n",
    "        indices = np.arange(numTrainItems)  # [0, 1, 2, . . n-1]  # rnd.shuffle(v)\n",
    "\n",
    "        while epoch < Num_Epochs:\n",
    "            self.rnd.shuffle(indices)  # SGD\n",
    "            for m in range(numTrainItems):\n",
    "                idx = indices[m]\n",
    "\n",
    "                for j in range(self.ni):\n",
    "                    X_Values[j] = trainData[idx, j]  # obtener los inputs\n",
    "                for j in range(self.no):\n",
    "                    T_Values[j] = trainData[idx, j+self.ni]  # obtener los targets\n",
    "\n",
    "                self.Feed_Forward(X_Values) # FeedForward\n",
    "\n",
    "                # 1. W_output\n",
    "                for k in range(self.no):\n",
    "                    derivative = (1 - self.oNodes[k]) * self.oNodes[k]  # softmax\n",
    "                    oSignals[k] = derivative * (self.oNodes[k] - T_Values[k])  # E=(t-o)^2 do E'=(o-t)\n",
    "\n",
    "                # 2. Hid2Out\n",
    "                for j in range(self.nh):\n",
    "                    for k in range(self.no):\n",
    "                        hoGrads[j, k] = oSignals[k] * self.hNodes[j]\n",
    "\n",
    "                # 3. b_output\n",
    "                for k in range(self.no):\n",
    "                    obGrads[k] = oSignals[k] * 1.0\n",
    "\n",
    "                # 4. W_Hidden\n",
    "                for j in range(self.nh):\n",
    "                    ssum = 0.0\n",
    "                    for k in range(self.no):\n",
    "                        ssum += oSignals[k] * self.hoWeights[j,k]\n",
    "                    derivative = (1 - self.hNodes[j]) * (1 + self.hNodes[j])  # tanh\n",
    "                    hSignals[j] = derivative * sum\n",
    "\n",
    "                # 5 Inp2Hid\n",
    "                for i in range(self.ni):\n",
    "                    for j in range(self.nh):\n",
    "                        ihGrads[i, j] = hSignals[j] * self.iNodes[i]\n",
    "\n",
    "                # 6. b_Hidden\n",
    "                for j in range(self.nh):\n",
    "                    hbGrads[j] = hSignals[j] * 1.0  # 1.0 dummy input can be dropped\n",
    "\n",
    "      ## -------------------------- update  -----------------------------\n",
    "\n",
    "                # 1. Inp2Hid\n",
    "                for i in range(self.ni):\n",
    "                    for j in range(self.nh):\n",
    "                        delta = -1.0 * Alpha * ihGrads[i,j]\n",
    "                        self.ihWeights[i, j] += delta\n",
    "\n",
    "                # 2. b_Hidden\n",
    "                for j in range(self.nh):\n",
    "                    delta = -1.0 * Alpha * hbGrads[j]\n",
    "                    self.hBiases[j] += delta\n",
    "\n",
    "                # 3. Hid2Out\n",
    "                for j in range(self.nh):\n",
    "                    for k in range(self.no):\n",
    "                        delta = -1.0 * Alpha * hoGrads[j,k]\n",
    "                        self.hoWeights[j, k] += delta\n",
    "\n",
    "                # 4. b_output\n",
    "                for k in range(self.no):\n",
    "                    delta = -1.0 * Alpha * obGrads[k]\n",
    "                    self.oBiases[k] += delta\n",
    "\n",
    "            epoch += 1\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                mse = self.meanSquaredError(trainData)\n",
    "                print(\"epoca = \" + str(epoch) + \" - mse = %0.4f \" % mse)\n",
    "\n",
    "        result = self.getWeights()\n",
    "        return result\n",
    "\n",
    "    def accuracy(self, tdata):\n",
    "        right_ans = 0; wrong_ans = 0\n",
    "        X_Values = np.zeros(shape=[self.ni], dtype=np.float32)\n",
    "        T_Values = np.zeros(shape=[self.no], dtype=np.float32)\n",
    "\n",
    "        for i in range(len(tdata)):\n",
    "            for j in range(self.ni):\n",
    "                X_Values[j] = tdata[i,j]\n",
    "            for j in range(self.no):\n",
    "                T_Values[j] = tdata[i, j+self.ni]\n",
    "\n",
    "            y_values = self.Feed_Forward(X_Values) # FeedForward\n",
    "            max_index = np.argmax(y_values)\n",
    "\n",
    "            if abs(T_Values[max_index] - 1.0) < 1.0e-5:\n",
    "                right_ans += 1\n",
    "            else:\n",
    "                wrong_ans += 1\n",
    "\n",
    "        return right_ans / (right_ans + wrong_ans)\n",
    "\n",
    "    def meanSquaredError(self, tdata):\n",
    "        sumSquaredError = 0.0\n",
    "        X_Values = np.zeros(shape=[self.ni], dtype=np.float32)\n",
    "        T_Values = np.zeros(shape=[self.no], dtype=np.float32)\n",
    "\n",
    "        for i in range(len(tdata)):\n",
    "            for j in range(self.ni):\n",
    "                X_Values[j] = tdata[i, j]\n",
    "            for k in range(self.no):\n",
    "                T_Values[k] = tdata[i, kj+self.ni]\n",
    "\n",
    "            y_values = self.Feed_Forward(X_Values) # FeedForward\n",
    "\n",
    "        return mean_squared_error(T_Values, y_values)\n",
    "    \n",
    "    @staticmethod\n",
    "    def hypertan(x):\n",
    "        if x < -20.0:\n",
    "            return -1.0\n",
    "        elif x > 20.0:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return math.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(oSums):\n",
    "        result = np.zeros(shape=[len(oSums)], dtype=np.float32)\n",
    "        m = max(oSums)\n",
    "        divisor = 0.0\n",
    "        for k in range(len(oSums)):\n",
    "             divisor += math.exp(oSums[k] - m)\n",
    "        for k in range(len(result)):\n",
    "            result[k] =  math.exp(oSums[k] - m) / divisor\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def totalWeights(nInput, nHidden, nOutput):\n",
    "        tw = (nInput * nHidden) + (nHidden * nOutput) + nHidden + nOutput\n",
    "        return tw\n",
    "\n",
    "##-----------------------------------------------------------------------------------\n",
    "##                                  MAIN\n",
    "##-----------------------------------------------------------------------------------\n",
    "def main():\n",
    "    Num_Input = 4\n",
    "    Num_Hidden = 5\n",
    "    Num_Output = 3\n",
    "    print(\"\\nRed Neuronal de arquitectura %d-%d-%d \\n\" %\n",
    "          (Num_Input, Num_Hidden, Num_Output) )\n",
    "    NN_BP = NeuralNetwork(Num_Input, Num_Hidden, Num_Output, seed = 3) # Red Objeto\n",
    "    \n",
    "    # Poner aqui los datos\n",
    "\n",
    "    Num_Epochs = 50\n",
    "    Alpha = 0.05\n",
    "\n",
    "    print(\"\\nPerformance en el entrenamiento \\n\")\n",
    "    NN_BP.train(trainDataMatrix, Num_Epochs, Alpha)\n",
    "    \n",
    "    accTrain = NN_BP.accuracy(trainDataMatrix)\n",
    "    accTest = NN_BP.accuracy(testDataMatrix)\n",
    "\n",
    "    print(\"\\nTrain accuracy = %0.4f \" % accTrain)\n",
    "    print(\"Test accuracy = %0.4f \\n\" % accTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         ngram_range = (1,2),\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "X_train = vect.transform(X_train_row)\n",
    "y_train = y_train_row\n",
    "\n",
    "for i in range(45):\n",
    "    clf.partial_fit(X_train[i:], y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your machine, it will take about 2-3 minutes to stream the documents and learn the weights for the logistic regression model to classify \"new\" movie reviews. Executing the preceding code, we used the first 45,000 movie reviews to train the classifier, which means that we have 5,000 reviews left for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy Partial_Fit: %.3f' % clf.score(X_test, y_test))\n",
    "#Exercise 3: compare  with your Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that the predictive performance, an accuracy of ~87%, is quite \"reasonable\" given that we \"only\" used the default parameters and didn't do any hyperparameter optimization. \n",
    "\n",
    "After we estimated the model perfomance, let us use those last 5,000 test samples to update our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we successfully trained a model to predict the sentiment of a movie review. Unfortunately, if we'd close this IPython notebook at this point, we'd have to go through the whole learning process again and again if we'd want to make a prediction on \"new data.\"\n",
    "\n",
    "So, to reuse this model, we could use the [`pickle`](https://docs.python.org/3.5/library/pickle.html) module to \"serialize a Python object structure\". Or even better, we could use the [`joblib`](https://pypi.python.org/pypi/joblib) library, which handles large NumPy arrays more efficiently.\n",
    "\n",
    "To install:\n",
    "conda install -c anaconda joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "if not os.path.exists('./pkl_objects'):\n",
    "    os.mkdir('./pkl_objects')\n",
    "    \n",
    "joblib.dump(vect, './vectorizer.pkl')\n",
    "joblib.dump(clf, './clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code above, we \"pickled\" the `HashingVectorizer` and the `SGDClassifier` so that we can re-use those objects later. However, `pickle` and `joblib` have a known issue with `pickling` objects or functions from a `__main__` block and we'd get an `AttributeError: Can't get attribute [x] on <module '__main__'>` if we'd unpickle it later. Thus, to pickle the `tokenizer` function, we can write it to a file and import it to get the `namespace` \"right\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# writefile tokenizer.py\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tokenizer import tokenizer\n",
    "joblib.dump(tokenizer, './tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us restart this IPython notebook and check if the we can load our serialized objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "tokenizer = joblib.load('./tokenizer.pkl')\n",
    "vect = joblib.load('./vectorizer.pkl')\n",
    "clf = joblib.load('./clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the `tokenizer`, `HashingVectorizer`, and the tranined logistic regression model, we can use it to make predictions on new data, which can be useful, for example, if we'd want to embed our classifier into a web application -- a topic for another IPython notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = ['I did not like this movie']\n",
    "X = vect.transform(example)\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = ['I loved this movie']\n",
    "X = vect.transform(example)\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
